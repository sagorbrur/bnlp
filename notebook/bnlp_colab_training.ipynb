{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bnlp_colab_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagorbrur/bnlp/blob/bnlp_4_dev/notebook/bnlp_colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BNLP\n",
        "\n",
        "BNLP is a natural language processing toolkit for Bengali Language. This tool will help you to tokenize Bengali text, Embedding Bengali words, Bengali POS Tagging, Construct Neural Model for Bengali NLP purposes.\n",
        "\n",
        "Here we are prodiving training approach of different model using **BNLP**"
      ],
      "metadata": {
        "id": "0SQ0x9bh9QsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "MuT4uyIf5-Gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# !pip install -U bnlp_toolkit\n",
        "!pip install bnlp-toolkit==4.0.0.dev4"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlp-toolkit==4.0.0.dev4\n",
            "  Downloading bnlp_toolkit-4.0.0.dev4-py3-none-any.whl (22 kB)\n",
            "Collecting sentencepiece (from bnlp-toolkit==4.0.0.dev4)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit==4.0.0.dev4) (4.3.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit==4.0.0.dev4) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit==4.0.0.dev4) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit==4.0.0.dev4) (1.10.1)\n",
            "Collecting sklearn-crfsuite (from bnlp-toolkit==4.0.0.dev4)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit==4.0.0.dev4) (4.66.0)\n",
            "Collecting ftfy (from bnlp-toolkit==4.0.0.dev4)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji==1.7.0 (from bnlp-toolkit==4.0.0.dev4)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bnlp-toolkit==4.0.0.dev4) (2.31.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->bnlp-toolkit==4.0.0.dev4) (0.2.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bnlp-toolkit==4.0.0.dev4) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit==4.0.0.dev4) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit==4.0.0.dev4) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bnlp-toolkit==4.0.0.dev4) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit==4.0.0.dev4) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit==4.0.0.dev4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit==4.0.0.dev4) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bnlp-toolkit==4.0.0.dev4) (2023.7.22)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite->bnlp-toolkit==4.0.0.dev4)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit==4.0.0.dev4) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->bnlp-toolkit==4.0.0.dev4) (0.9.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=97760af148270953ae7148c41fafbe382f5d397c2613ef4f7e1534ff34cf8775\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: sentencepiece, python-crfsuite, emoji, sklearn-crfsuite, ftfy, bnlp-toolkit\n",
            "Successfully installed bnlp-toolkit-4.0.0.dev4 emoji-1.7.0 ftfy-6.1.1 python-crfsuite-0.9.9 sentencepiece-0.1.99 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "metadata": {
        "id": "KJN642aj5nVc",
        "outputId": "f51e370b-2244-4a67-a3d6-e63a0f2cb462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Download Bengali Wiki data"
      ],
      "metadata": {
        "id": "IWy0qUdy6BY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\"\n",
        "output = \"bn_wiki_data.txt.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "!unzip bn_wiki_data.txt.zip\n",
        "!rm -rf bn_wiki_data.txt.zip"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\n",
            "To: /content/bn_wiki_data.txt.zip\n",
            "100%|██████████| 69.2M/69.2M [00:00<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  bn_wiki_data.txt.zip\n",
            "  inflating: bn_wiki_data.txt        \n"
          ]
        }
      ],
      "metadata": {
        "id": "AcwFE8le5yTF",
        "outputId": "73ed584d-b40f-4532-aa07-59cd48cac2c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Text Data"
      ],
      "metadata": {
        "id": "ywTQFAeU6QEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sample.txt\n",
        "চলতি সপ্তাহের শেষে সর্বজনীন পেনশন কর্মসূচির উদ্বোধন হতে যাচ্ছে। প্রধানমন্ত্রীর কার্যালয় থেকে অর্থ মন্ত্রণালয়ে পাঠানো চিঠিতে বলা হয়েছে, প্রধানমন্ত্রী শেখ হাসিনা আগামী ১৭ আগস্ট বৃহস্পতিবার ভার্চ্যুয়াল পদ্ধতিতে এ কর্মসূচির উদ্বোধন করবেন।\n",
        "\n",
        "ওই দিন সকাল ১০টায় ঢাকায় গণভবন থেকে প্রধানমন্ত্রী পেনশন কর্মসূচির উদ্বোধন করবেন। অনুষ্ঠানে সংযুক্ত থাকবে গোপালগঞ্জ, বাগেরহাট, রংপুর জেলা ও সৌদি আরবের বাংলাদেশ দূতাবাস।\n",
        "এর আগে অর্থ বিভাগ সূত্র প্রথম আলোকে জানিয়েছিল, সমাজের বিভিন্ন শ্রেণি-পেশার মানুষের কথা বিবেচনায় নিয়ে সর্বজনীন পেনশন কর্মসূচি চালু করা হচ্ছে। আপাতত চার শ্রেণির জনগোষ্ঠীর জন্য চার ধরনের পেনশন কর্মসূচি চালু করা হচ্ছে। এগুলোর নাম হচ্ছে প্রগতি, সুরক্ষা, সমতা ও প্রবাসী।\n",
        "\n",
        "\n",
        "এর মধ্যে বেসরকারি খাতের চাকরিজীবীদের জন্য ‘প্রগতি’, স্বকর্মে নিয়োজিত ব্যক্তিদের জন্য ‘সুরক্ষা’, প্রবাসী বাংলাদেশিদের জন্য ‘প্রবাসী’ ও দেশের নিম্ন আয়ের জনগোষ্ঠীর জন্য ‘সমতা’ শীর্ষক কর্মসূচি চালু করা হবে।\n",
        "\n",
        "সর্বজনীন পেনশন কর্মসূচি এমনভাবে করা হচ্ছে, যাতে দেশের ১৮ থেকে ৫০ বছর বয়সী সব নাগরিকই এ ব্যবস্থার আওতায় আসতে পারেন এবং ৬০ বছর বয়স থেকে তাঁরা আজীবন পেনশন পাবেন। শুরুর দিকে চিন্তা না থাকলেও পরে ৫০ বছরের বেশি বয়সীদেরও পেনশন কর্মসূচির আওতায় রাখার সুযোগ তৈরি করা হয়েছে। তাঁরা টানা ১০ বছর চাঁদা দেওয়ার পর পেনশন সুবিধা পাবেন।\n",
        "\n"
      ],
      "metadata": {
        "id": "sL96g8j16PWG",
        "outputId": "5c31e464-ac4e-4e07-aeae-369b981feb0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sample.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Here we present `bengali sentencepiece`, `bengali word2vec`, `bengali fasttext` training on `bengali wikipedia data`\n",
        "\n",
        "Training time will depend on data size."
      ],
      "metadata": {
        "id": "350KPo4D6Z4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Sentencepice Model\n",
        "\n",
        "After successfully compiling the below code will produce two file:\n",
        "\n",
        "* `wiki_sp.model`\n",
        "* `wiki_sp.vecab`"
      ],
      "metadata": {
        "id": "I_wHJFOW6dlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from bnlp import SentencepieceTrainer\n",
        "\n",
        "data = \"sample.txt\"\n",
        "vocab_size = 100\n",
        "model_prefix = \"model\"\n",
        "\n",
        "trainer = SentencepieceTrainer(\n",
        "   data=data,\n",
        "   vocab_size=vocab_size,\n",
        "   model_prefix=model_prefix\n",
        ")\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.model and model.vocab is saved on your current directory\n"
          ]
        }
      ],
      "metadata": {
        "id": "8l7DUWI66MD4",
        "outputId": "89201111-80d5-42d0-bdc8-d31ab4657229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Word2Vec Model\n",
        "\n",
        "After successfully compiling it will produce three file.\n",
        "\n",
        "* `wiki_word2vec.model`\n",
        "* `wiki_word2vec.vector`\n",
        "* `wiki_word2vec.model.trainables.syn1neg.npy`\n",
        "* `wiki_word2vec..model.wv.vectors.npy`\n"
      ],
      "metadata": {
        "id": "k-k4Dszo61v2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "from bnlp import Word2VecTraining\n",
        "\n",
        "trainer = Word2VecTraining()\n",
        "\n",
        "data_file = \"sample.txt\" # or you can pass custom sentence tokens as list of list\n",
        "model_name = \"test_model.model\"\n",
        "vector_name = \"test_vector.vector\"\n",
        "trainer.train(data_file, model_name, vector_name, epochs=5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started.......\n",
            "please wait.....it will take time according to your data size and computation capability\n",
            "train completed successfully\n",
            "trianing loss: 44.43490219116211\n",
            "model and vector saving...\n",
            "model and vector saved as test_model.model and test_vector.vector\n"
          ]
        }
      ],
      "metadata": {
        "id": "OphHV5Yp60KW",
        "outputId": "7af25b9a-b53d-4f09-9e8d-02e8e70b82da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-training or resume Bengali word2vec training"
      ],
      "metadata": {
        "id": "wdMRUO0jzV8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "from bnlp import Word2VecTraining\n",
        "\n",
        "trainer = Word2VecTraining()\n",
        "\n",
        "trained_model_path = \"test_model.model\"\n",
        "data_file = \"sample.txt\"\n",
        "model_name = \"test_model.model\"\n",
        "vector_name = \"test_vector.vector\"\n",
        "trainer.pretrain(trained_model_path, data_file, model_name, vector_name, epochs=5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loading ....\n",
            "vocab building with new sentences\n",
            "pre-training started.......\n",
            "please wait.....it will take time according to your data size and computation capability\n",
            "pre-train completed successfully\n",
            "pre-trianing loss: 0.0\n",
            "model and vector saving...\n",
            "model and vector saved as test_model.model and test_vector.vector\n"
          ]
        }
      ],
      "metadata": {
        "id": "i1aTn9cnzV8v",
        "outputId": "ab71f451-e873-4867-8a9c-f0800a034d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali Fasttext Model\n",
        "First of all install `fasttext` using `pip install fasttext` and restart runtime.\n",
        "\n",
        "After successfully training it will produce:\n",
        "* `wiki_fasttext.bin`"
      ],
      "metadata": {
        "id": "TAMgr4WT8x2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "!pip install fasttext"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199770 sha256=c71df6599369022ad45445efe631c0f692a67fa2b1e78ca6ca3c0d04d4ca944c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ],
      "metadata": {
        "id": "JXptOhxg4s6r",
        "outputId": "8ef5c7cf-947a-49db-8518-378457cd55fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "from bnlp.embedding.fasttext import FasttextTrainer\n",
        "\n",
        "trainer = FasttextTrainer()\n",
        "\n",
        "data = \"sample.txt\"\n",
        "model_name = \"saved_model.bin\"\n",
        "epoch = 5\n",
        "trainer.train(data, model_name, epoch)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training started.....\n",
            "training done! saving as saved_model.bin\n"
          ]
        }
      ],
      "metadata": {
        "id": "F67Yzdu08xBd",
        "outputId": "48c61612-9c72-4995-a5ce-3e3c7acfd01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bengali Doc2Vec"
      ],
      "metadata": {
        "id": "C_D6o84iz9le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import BengaliDoc2vecTrainer\n",
        "\n",
        "trainer = BengaliDoc2vecTrainer()\n",
        "\n",
        "text_files = \"./\"\n",
        "checkpoint_path = \"logs\"\n",
        "\n",
        "trainer.train(\n",
        "  text_files,\n",
        "  checkpoint_path=checkpoint_path,\n",
        "  vector_size=100,\n",
        "  min_count=2,\n",
        "  epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "5yNxSzNq0Al1",
        "outputId": "8ed3238e-fed4-44e8-e986-f8c344f4d48e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00, 454.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Bengali POS TAGGING CRF model\n",
        "\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data:\n",
        "\n",
        "* `pos_model.pkl`"
      ],
      "metadata": {
        "id": "ZtsLVmOs9lgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "from bnlp import CRFTaggerTrainer\n",
        "\n",
        "trainer = CRFTaggerTrainer()\n",
        "\n",
        "model_name = \"pos_model.pkl\"\n",
        "train_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা',  'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "\n",
        "test_data = [[('রপ্তানি', 'JJ'), ('দ্রব্য', 'NC'), ('-', 'PU'), ('তাজা', 'JJ'), ('ও', 'CCD'), ('শুকনা', 'JJ'), ('ফল', 'NC'), (',', 'PU'), ('আফিম', 'NC'), (',', 'PU'), ('পশুচর্ম', 'NC'), ('ও', 'CCD'), ('পশম', 'NC'), ('এবং', 'CCD'),('কার্পেট', 'NC'), ('৷', 'PU')], [('মাটি', 'NC'), ('থেকে', 'PP'), ('বড়জোর', 'JQ'), ('চার', 'JQ'), ('পাঁচ', 'JQ'), ('ফুট', 'CCL'), ('উঁচু', 'JJ'), ('হবে', 'VM'), ('৷', 'PU')]]\n",
        "\n",
        "trainer.train(model_name, train_data, test_data)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "Training Started........\n",
            "It will take time according to your dataset size...\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "1.0\n",
            "F1 Score(micro) is: \n",
            "1.0\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "VUKhbkaBE-CV",
        "outputId": "378b73d1-dc4d-4260-f0a4-39d9b2da0e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Bengali NER model\n",
        "After successfully training it will produce a trained model with accuracy on evaluation data:\n",
        "\n",
        "* `ner_model.pkl`"
      ],
      "metadata": {
        "id": "dPB7SBrKuSna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "from bnlp import CRFTaggerTrainer\n",
        "\n",
        "trainer = CRFTaggerTrainer()\n",
        "\n",
        "model_name = \"ner_model.pkl\"\n",
        "train_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "\n",
        "test_data = [[('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')], [('ত্রাণ', 'O'),('ও', 'O'),('সমাজকল্যাণ', 'O'),('সম্পাদক', 'S-PER'),('সুজিত', 'B-PER'),('রায়', 'I-PER'),('নন্দী', 'E-PER'),('প্রমুখ', 'O'),('সংবাদ', 'O'),('সম্মেলনে', 'O'),('উপস্থিত', 'O'),('ছিলেন', 'O')]]\n",
        "\n",
        "trainer.train(model_name, train_data, test_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "3\n",
            "Training Started........\n",
            "It will take time according to your dataset size...\n",
            "Training Finished!\n",
            "Evaluating with Test Data...\n",
            "Accuracy is: \n",
            "1.0\n",
            "F1 Score(micro) is: \n",
            "1.0\n",
            "Model Saved!\n"
          ]
        }
      ],
      "metadata": {
        "id": "of_1lkdW917n",
        "outputId": "f877b62a-b568-4d45-81ae-b4e770a95ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "qVrYxT5DulwP"
      }
    }
  ]
}