{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bnlp_colab_training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzTwEVaDUTKlImUgNv0gK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagorbrur/bnlp/blob/master/notebook/bnlp_colab_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SQ0x9bh9QsL",
        "colab_type": "text"
      },
      "source": [
        "# BNLP\n",
        "\n",
        "BNLP is a natural language processing toolkit for Bengali Language. This tool will help you to tokenize Bengali text, Embedding Bengali words, Bengali POS Tagging, Construct Neural Model for Bengali NLP purposes.\n",
        "\n",
        "Here we are prodiving training approach of different model using **BNLP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuT4uyIf5-Gy",
        "colab_type": "text"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJN642aj5nVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "0f87377e-22da-4818-e87d-4fadcea15767"
      },
      "source": [
        "!pip install bnlp_toolkit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/09/74e8234af0848efc6c56d7aeb30b9e88d360b988867c3ce1f8bfbf28364e/bnlp_toolkit-2.2-py3-none-any.whl\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (3.2.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (3.6.0)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bnlp_toolkit) (1.18.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext->bnlp_toolkit) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext->bnlp_toolkit) (46.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->bnlp_toolkit) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bnlp_toolkit) (1.11.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (4.38.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.8.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bnlp_toolkit) (2.21.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bnlp_toolkit) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bnlp_toolkit) (1.12.40)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->bnlp_toolkit) (2020.4.5.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bnlp_toolkit) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bnlp_toolkit) (1.15.40)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bnlp_toolkit) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->smart-open>=1.2.1->gensim->bnlp_toolkit) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->smart-open>=1.2.1->gensim->bnlp_toolkit) (2.8.1)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2390048 sha256=ed42f1ed81346d3692e6c3051b7f6eda83db9457e4e5ea8cbafb87864244f13c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext, python-crfsuite, sklearn-crfsuite, sentencepiece, bnlp-toolkit\n",
            "Successfully installed bnlp-toolkit-2.2 fasttext-0.9.1 python-crfsuite-0.9.7 sentencepiece-0.1.85 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWy0qUdy6BY3",
        "colab_type": "text"
      },
      "source": [
        "##  Downloading Bengali Processed Wikipedia Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcwFE8le5yTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ba4cbc94-aa67-43db-fdcc-63dceb69bb82"
      },
      "source": [
        "#drive data download code\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "downloaded = drive.CreateFile({'id':\"1rQUQLsXg0TZnlrAgmNMkCXGDnYbjlLmM\"})\n",
        "downloaded.GetContentFile('bn_wiki_data.txt.zip')\n",
        "\n",
        "!unzip bn_wiki_data.txt.zip\n",
        "!rm -rf bn_wiki_data.txt.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bn_wiki_data.txt.zip\n",
            "  inflating: bn_wiki_data.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "350KPo4D6Z4o",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Here we present `bengali sentencepiece`, `bengali word2vec`, `bengali fasttext` training on `bengali wikipedia data`\n",
        "\n",
        "Training time will depend on data size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_wHJFOW6dlo",
        "colab_type": "text"
      },
      "source": [
        "### Training Bengali Sentencepice Model\n",
        "\n",
        "After successfully compiling the below code will produce two file:\n",
        "\n",
        "* `wiki_sp.model` \n",
        "* `wiki_sp.vecab`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l7DUWI66MD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3727978f-fe92-4fbf-e2f0-953ab60705af"
      },
      "source": [
        "from bnlp.sentencepiece_tokenizer import SP_Tokenizer\n",
        "\n",
        "bsp = SP_Tokenizer()\n",
        "data = \"bn_wiki_data.txt\"\n",
        "model_prefix = \"wiki_sp\"\n",
        "vocab_size = 30000\n",
        "bsp.train_bsp(data, model_prefix, vocab_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "punkt not found. downloading...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "wiki_sp.model and wiki_sp.vocab is saved on your current directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-k4Dszo61v2",
        "colab_type": "text"
      },
      "source": [
        "### Training Bengali Word2Vec Model\n",
        "\n",
        "After successfully compiling it will produce three file. \n",
        "\n",
        "* `wiki_word2vec.model`\n",
        "* `wiki_word2vec.model.trainables.syn1neg.npy`\n",
        "* `wiki_word2vec..model.wv.vectors.npy`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OphHV5Yp60KW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "54e5626d-3b72-476c-b8a1-1b891b511797"
      },
      "source": [
        "from bnlp.bengali_word2vec import Bengali_Word2Vec\n",
        "bwv = Bengali_Word2Vec(True)\n",
        "data_file = \"bn_wiki_data.txt\"\n",
        "model_name = \"wiki_word2vec.model\"\n",
        "vector_name = \"wiki_word2vec.vector\"\n",
        "bwv.train_word2vec(data_file, model_name, vector_name)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wiki_word2vec.model and wiki_word2vec.vector saved in your current directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAMgr4WT8x2a",
        "colab_type": "text"
      },
      "source": [
        "### Training Bengali Fasttext Model\n",
        "\n",
        "After successfully training it will produce: \n",
        "* `wiki_fasttext.bin` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F67Yzdu08xBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bnlp.bengali_fasttext import Bengali_Fasttext\n",
        "\n",
        "bft = Bengali_Fasttext()\n",
        "data = \"bn_wiki_data.txt\"\n",
        "model_name = \"wiki_fasttext.bin\"\n",
        "epoch = 1\n",
        "bft.train_fasttext(data, model_name, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUKhbkaBE-CV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}