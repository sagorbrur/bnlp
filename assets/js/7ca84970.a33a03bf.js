"use strict";(self.webpackChunkbnlp=self.webpackChunkbnlp||[]).push([[958],{987:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>p,frontMatter:()=>d,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"tokenization","title":"Tokenization","description":"Basic Tokenizer","source":"@site/docs/4-tokenization.md","sourceDirName":".","slug":"/tokenization","permalink":"/bnlp/docs/tokenization","draft":false,"unlisted":false,"editUrl":"https://github.com/sagorbrur/bnlp-doc/tree/main/docs/4-tokenization.md","tags":[],"version":"current","lastUpdatedBy":"Ariful Alam","lastUpdatedAt":1733130906000,"sidebarPosition":4,"frontMatter":{"id":"tokenization","title":"Tokenization"},"sidebar":"tutorialSidebar","previous":{"title":"Pretrained Model","permalink":"/bnlp/docs/pretrained-model"},"next":{"title":"Word Embedding","permalink":"/bnlp/docs/word-embedding"}}');var o=t(4848),r=t(8453);const d={id:"tokenization",title:"Tokenization"},a=void 0,s={},l=[{value:"Basic Tokenizer",id:"basic-tokenizer",level:2},{value:"NLTK Tokenization",id:"nltk-tokenization",level:2},{value:"Bengali SentencePiece Tokenization",id:"bengali-sentencepiece-tokenization",level:2},{value:"Tokenization using trained model",id:"tokenization-using-trained-model",level:3},{value:"Tokenization Using Own Model",id:"tokenization-using-own-model",level:3},{value:"Training SentencePiece",id:"training-sentencepiece",level:3}];function c(e){const n={code:"code",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"basic-tokenizer",children:"Basic Tokenizer"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'from bnlp import BasicTokenizer\n\ntokenizer = BasicTokenizer()\n\nraw_text = "\u0986\u09ae\u09bf \u09ac\u09be\u0982\u09b2\u09be\u09df \u0997\u09be\u09a8 \u0997\u09be\u0987\u0964"\ntokens = tokenizer(raw_text)\nprint(tokens)\n# output: ["\u0986\u09ae\u09bf", "\u09ac\u09be\u0982\u09b2\u09be\u09df", "\u0997\u09be\u09a8", "\u0997\u09be\u0987", "\u0964"]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"nltk-tokenization",children:"NLTK Tokenization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'from bnlp import NLTKTokenizer\n\nbnltk = NLTKTokenizer()\n\ntext = "\u0986\u09ae\u09bf \u09ad\u09be\u09a4 \u0996\u09be\u0987\u0964 \u09b8\u09c7 \u09ac\u09be\u099c\u09be\u09b0\u09c7 \u09af\u09be\u09df\u0964 \u09a4\u09bf\u09a8\u09bf \u0995\u09bf \u09b8\u09a4\u09cd\u09af\u09bf\u0987 \u09ad\u09be\u09b2\u09cb \u09ae\u09be\u09a8\u09c1\u09b7?"\nword_tokens = bnltk.word_tokenize(text)\nsentence_tokens = bnltk.sentence_tokenize(text)\nprint(word_tokens)\nprint(sentence_tokens)\n# output\n# word_token: ["\u0986\u09ae\u09bf", "\u09ad\u09be\u09a4", "\u0996\u09be\u0987", "\u0964", "\u09b8\u09c7", "\u09ac\u09be\u099c\u09be\u09b0\u09c7", "\u09af\u09be\u09df", "\u0964", "\u09a4\u09bf\u09a8\u09bf", "\u0995\u09bf", "\u09b8\u09a4\u09cd\u09af\u09bf\u0987", "\u09ad\u09be\u09b2\u09cb", "\u09ae\u09be\u09a8\u09c1\u09b7", "?"]\n# sentence_token: ["\u0986\u09ae\u09bf \u09ad\u09be\u09a4 \u0996\u09be\u0987\u0964", "\u09b8\u09c7 \u09ac\u09be\u099c\u09be\u09b0\u09c7 \u09af\u09be\u09df\u0964", "\u09a4\u09bf\u09a8\u09bf \u0995\u09bf \u09b8\u09a4\u09cd\u09af\u09bf\u0987 \u09ad\u09be\u09b2\u09cb \u09ae\u09be\u09a8\u09c1\u09b7?"]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"bengali-sentencepiece-tokenization",children:"Bengali SentencePiece Tokenization"}),"\n",(0,o.jsx)(n.h3,{id:"tokenization-using-trained-model",children:"Tokenization using trained model"}),"\n",(0,o.jsxs)(n.p,{children:["To use pretrained model do not pass ",(0,o.jsx)(n.code,{children:"model_path"})," to ",(0,o.jsx)(n.code,{children:"SentencepieceTokenizer()"}),". It will download pretrained ",(0,o.jsx)(n.code,{children:"SentencepieceTokenizer"})," model itself."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'from bnlp import SentencepieceTokenizer\n\nbsp = SentencepieceTokenizer()\n\n\ninput_text = "\u0986\u09ae\u09bf \u09ad\u09be\u09a4 \u0996\u09be\u0987\u0964 \u09b8\u09c7 \u09ac\u09be\u099c\u09be\u09b0\u09c7 \u09af\u09be\u09df\u0964"\ntokens = bsp.tokenize(input_text)\nprint(tokens)\ntext2id = bsp.text2id(input_text)\nprint(text2id)\nid2text = bsp.id2text(text2id)\nprint(id2text)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"tokenization-using-own-model",children:"Tokenization Using Own Model"}),"\n",(0,o.jsxs)(n.p,{children:["To use own model pass model path as ",(0,o.jsx)(n.code,{children:"model_path"})," argument to ",(0,o.jsx)(n.code,{children:"SentencepieceTokenizer()"})," like below snippet."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'from bnlp import SentencepieceTokenizer\n\nown_model_path = "own_directory/own_sp_model.pkl"\nbsp = SentencepieceTokenizer(model_path=own_model_path)\n\n\ninput_text = "\u0986\u09ae\u09bf \u09ad\u09be\u09a4 \u0996\u09be\u0987\u0964 \u09b8\u09c7 \u09ac\u09be\u099c\u09be\u09b0\u09c7 \u09af\u09be\u09df\u0964"\ntokens = bsp.tokenize(input_text)\nprint(tokens)\ntext2id = bsp.text2id(input_text)\nprint(text2id)\nid2text = bsp.id2text(text2id)\nprint(id2text)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"training-sentencepiece",children:"Training SentencePiece"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-py",children:'from bnlp import SentencepieceTrainer\n\ndata = "raw_text.txt"\nvocab_size = 32000\nmodel_prefix = "model"\n\ntrainer = SentencepieceTrainer(\n   data=data,\n   vocab_size=vocab_size,\n   model_prefix=model_prefix\n)\ntrainer.train()\n\n'})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>a});var i=t(6540);const o={},r=i.createContext(o);function d(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:d(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);