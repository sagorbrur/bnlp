"use strict";(self.webpackChunkbnlp=self.webpackChunkbnlp||[]).push([[628],{8531:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>a});const l=JSON.parse('{"id":"archive/v1.2.0","title":"v1.2.0","description":"BNLP is a natural language processing toolkit for Bengali Language. This tool will help you to tokenize Bengali text, Embedding Bengali words, construct neural model for Bengali NLP purposes.","source":"@site/docs/11-archive/2-v1.2.0.md","sourceDirName":"11-archive","slug":"/archive/v1.2.0","permalink":"/bnlp/docs/archive/v1.2.0","draft":false,"unlisted":false,"editUrl":"https://github.com/sagorbrur/bnlp-doc/tree/main/docs/11-archive/2-v1.2.0.md","tags":[],"version":"current","lastUpdatedBy":"Ariful Alam","lastUpdatedAt":1733208908000,"sidebarPosition":2,"frontMatter":{"title":"v1.2.0","hide_title":true},"sidebar":"tutorialSidebar","previous":{"title":"v1.0.0","permalink":"/bnlp/docs/archive/v1.0.0"},"next":{"title":"v2.3.0","permalink":"/bnlp/docs/archive/v2.3.0"}}');var r=i(4848),t=i(8453);const s={title:"v1.2.0",hide_title:!0},o="Bengali Natural Language Processing(BNLP)",d={},a=[{value:"Current Features",id:"current-features",level:2},{value:"Installation",id:"installation",level:2},{value:"Pretrained Model",id:"pretrained-model",level:2},{value:"Download Link",id:"download-link",level:3},{value:"Training Details",id:"training-details",level:3},{value:"Tokenization",id:"tokenization",level:2},{value:"Word Embedding",id:"word-embedding",level:2},{value:"Issue",id:"issue",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"bengali-natural-language-processingbnlp",children:"Bengali Natural Language Processing(BNLP)"})}),"\n",(0,r.jsxs)(n.p,{children:["BNLP is a natural language processing toolkit for Bengali Language. This tool will help you to ",(0,r.jsx)(n.strong,{children:"tokenize Bengali text"}),", ",(0,r.jsx)(n.strong,{children:"Embedding Bengali words"}),", ",(0,r.jsx)(n.strong,{children:"construct neural model"})," for Bengali NLP purposes."]}),"\n",(0,r.jsx)(n.h2,{id:"current-features",children:"Current Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"#tokenization",children:"Bengali Tokenization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"SentencePiece Tokenizer"}),"\n",(0,r.jsx)(n.li,{children:"Basic Tokenizer"}),"\n",(0,r.jsx)(n.li,{children:"NLTK Tokenizer"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"#word-embedding",children:"Bengali Word Embedding"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Bengali Word2Vec"}),"\n",(0,r.jsx)(n.li,{children:"Bengali Fasttext"}),"\n",(0,r.jsx)(n.li,{children:"Bengali GloVe"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"pypi package installer(python 3.5, 3.6, 3.7 tested okay)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"pip install bnlp_toolkit"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Local"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"$git clone https://github.com/sagorbrur/bnlp.git\n$cd bnlp\n$python setup.py install\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"pretrained-model",children:"Pretrained Model"}),"\n",(0,r.jsx)(n.h3,{id:"download-link",children:"Download Link"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/sagorbrur/bnlp/tree/master/model",children:"Bengali SentencePiece"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://drive.google.com/open?id=1DxR8Vw61zRxuUm17jzFnOX97j7QtNW7U",children:"Bengali Word2Vec"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://drive.google.com/open?id=1CFA-SluRyz3s5gmGScsFUcs7AjLfscm2",children:"Bengali FastText"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/sagorbrur/GloVe-Bengali",children:"Bengali GloVe Wordvectors"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"training-details",children:"Training Details"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["All three model trained with ",(0,r.jsx)(n.strong,{children:"Bengali Wikipedia Dump Dataset"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://dumps.wikimedia.org/bnwiki/latest/",children:"Bengali Wiki Dump"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"SentencePiece Training Vocab Size=50000"}),"\n",(0,r.jsx)(n.li,{children:"Fasttext trained with total words = 20M, vocab size = 1171011, epoch=50, embedding dimension = 300 and the training loss = 0.318668,"}),"\n",(0,r.jsx)(n.li,{children:"Word2Vec word embedding dimension = 300"}),"\n",(0,r.jsxs)(n.li,{children:["To Know Bengali GloVe Wordvector and training process follow ",(0,r.jsx)(n.a,{href:"https://github.com/sagorbrur/GloVe-Bengali",children:"this"})," repository"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"tokenization",children:"Tokenization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bengali SentencePiece Tokenization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["tokenization using trained model","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.sentencepiece_tokenizer import SP_Tokenizer\n\nbsp = SP_Tokenizer()\nmodel_path = "./model/bn_spm.model"\ninput_text = "\u0986\u09ae\u09bf \u09ad\u09be\u09a4 \u0996\u09be\u0987\u0964 \u09b8\u09c7 \u09ac\u09be\u099c\u09be\u09b0\u09c7 \u09af\u09be\u09df\u0964"\ntokens = bsp.tokenize(model_path, input_text)\nprint(tokens)\n\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Training SentencePiece","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.sentencepiece_tokenizer import SP_Tokenizer\n\nbsp = SP_Tokenizer(is_train=True)\ndata = "test.txt"\nmodel_prefix = "test"\nvocab_size = 5\nbsp.train_bsp(data, model_prefix, vocab_size) \n\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Basic Tokenizer"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.basic_tokenizer import BasicTokenizer\nbasic_t = BasicTokenizer(False)\nraw_text = "\u0986\u09ae\u09bf \u09ac\u09be\u0982\u09b2\u09be\u09df \u0997\u09be\u09a8 \u0997\u09be\u0987\u0964"\ntokens = basic_t.tokenize(raw_text)\nprint(tokens)\n\n# output: ["\u0986\u09ae\u09bf", "\u09ac\u09be\u0982\u09b2\u09be\u09df", "\u0997\u09be\u09a8", "\u0997\u09be\u0987", "\u0964"]\n\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"NLTK Tokenization"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.nltk_tokenizer import NLTK_Tokenizer\n\ntext = "\u0986\u09ae\u09bf \u09ad\u09be\u09a4 \u0996\u09be\u0987\u0964 \u09b8\u09c7 \u09ac\u09be\u099c\u09be\u09b0\u09c7 \u09af\u09be\u09df\u0964 \u09a4\u09bf\u09a8\u09bf \u0995\u09bf \u09b8\u09a4\u09cd\u09af\u09bf\u0987 \u09ad\u09be\u09b2\u09cb \u09ae\u09be\u09a8\u09c1\u09b7?"\nbnltk = NLTK_Tokenizer(text)\nword_tokens = bnltk.word_tokenize()\nsentence_tokens = bnltk.sentence_tokenize()\nprint(word_tokens)\nprint(sentence_tokens)\n\n# output\n# word_token: ["\u0986\u09ae\u09bf", "\u09ad\u09be\u09a4", "\u0996\u09be\u0987", "\u0964", "\u09b8\u09c7", "\u09ac\u09be\u099c\u09be\u09b0\u09c7", "\u09af\u09be\u09df", "\u0964", "\u09a4\u09bf\u09a8\u09bf", "\u0995\u09bf", "\u09b8\u09a4\u09cd\u09af\u09bf\u0987", "\u09ad\u09be\u09b2\u09cb", "\u09ae\u09be\u09a8\u09c1\u09b7", "?"]\n# sentence_token: ["\u0986\u09ae\u09bf \u09ad\u09be\u09a4 \u0996\u09be\u0987\u0964", "\u09b8\u09c7 \u09ac\u09be\u099c\u09be\u09b0\u09c7 \u09af\u09be\u09df\u0964", "\u09a4\u09bf\u09a8\u09bf \u0995\u09bf \u09b8\u09a4\u09cd\u09af\u09bf\u0987 \u09ad\u09be\u09b2\u09cb \u09ae\u09be\u09a8\u09c1\u09b7?"]\n\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"word-embedding",children:"Word Embedding"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bengali Word2Vec"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Generate Vector using pretrain model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"from bnlp.bengali_word2vec import Bengali_Word2Vec\n\nbwv = Bengali_Word2Vec()\nmodel_path = \"model/bengali_word2vec.model\"\nword = '\u0986\u09ae\u09be\u09b0'\nvector = bwv.generate_word_vector(model_path, word)\nprint(vector.shape)\nprint(vector)\n\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Find Most Similar Word Using Pretrained Model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"from bnlp.bengali_word2vec import Bengali_Word2Vec\n\nbwv = Bengali_Word2Vec()\nmodel_path = \"model/bengali_word2vec.model\"\nword = '\u0986\u09ae\u09be\u09b0'\nsimilar = bwv.most_similar(model_path, word)\nprint(similar)\n\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Train Bengali Word2Vec with your own data"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.bengali_word2vec import Bengali_Word2Vec\nbwv = Bengali_Word2Vec(is_train=True)\ndata_file = "test.txt"\nmodel_name = "test_model.model"\nvector_name = "test_vector.vector"\nbwv.train_word2vec(data_file, model_name, vector_name)\n\n\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bengali FastText"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Generate Vector Using Pretrained Model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.bengali_fasttext import Bengali_Fasttext\n\nbft = Bengali_Fasttext()\nword = "\u0997\u09cd\u09b0\u09be\u09ae"\nmodel_path = "model/bengali_fasttext.bin"\nword_vector = bft.generate_word_vector(model_path, word)\nprint(word_vector.shape)\nprint(word_vector)\n\n\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Train Bengali FastText Model"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.bengali_fasttext import Bengali_Fasttext\n\nbft = Bengali_Fasttext(is_train=True)\ndata = "data.txt"\nmodel_name = "saved_model.bin"\nepoch = 50\nbft.train_fasttext(data, model_name, epoch) # epoch not implement in pypi yet\n# bft.train_fasttext(data, model_name) in pypi now\n\n'})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Bengali GloVe Word Vectors"})}),"\n",(0,r.jsxs)(n.p,{children:["We trained glove model with bengali data(wiki+news articles) and published bengali glove word vectors",(0,r.jsx)("br",{}),"\nYou can download and use it on your different machine learning purposes."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'from bnlp.glove_wordvector import BN_Glove\nglove_path = "bn_glove.39M.100d.txt"\nword = "\u0997\u09cd\u09b0\u09be\u09ae"\nbng = BN_Glove()\nres = bng.closest_word(glove_path, word)\nprint(res)\nvec = bng.word2vec(glove_path, word)\nprint(vec)\n\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"issue",children:"Issue"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["if ",(0,r.jsx)(n.code,{children:"ModuleNotFoundError: No module named 'fasttext'"})," problem arise please do the next line"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"pip install fasttext"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["if ",(0,r.jsx)(n.code,{children:"nltk"})," issue arise please do the following line before importing ",(0,r.jsx)(n.code,{children:"bnlp"})]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:'import nltk\nnltk.download("punkt")\n'})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var l=i(6540);const r={},t=l.createContext(r);function s(e){const n=l.useContext(t);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),l.createElement(t.Provider,{value:n},e.children)}}}]);